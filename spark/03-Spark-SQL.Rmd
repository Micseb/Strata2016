---
title: "R with Big Data 3: Big Data and Spark SQL"
author: "Garrett Grolemund and Nathan Stephens"
date: "September 27, 2016"
output: html_notebook
---

```{r setup, include = FALSE}
library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)
```

These are the class notes (Part 3 of 3) for *R for Big Data*, a workshop taught at *Strata + Hadoop World 2016 NYC*. The notes are saved as an R Markdown Notebook. See Part 1, *Universal Tools* to learn more about how to use R Markdown Notebooks.

### Flights Data

This guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the `nycflights13` R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US [Bureau of Transportation Statistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0), and is documented in `?nycflights13`

### Connect and load data

Connect to the cluster and copy the flights data using the `copy_to` function. Caveat: The flight data in `nycflights13` is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects. 

```{r message=FALSE, warning=FALSE}
sc <- spark_connect(master = "local", version = "2.0.0", hadoop_version="2.7")
flights_tbl <- copy_to(sc, flights, "flights")
airlines_tbl <- copy_to(sc, airlines, "airlines")
src_tbls(sc)
```

## dplyr Verbs

Verbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:

* `select` ~ `SELECT`
* `filter` ~ `WHERE`
* `arrange` ~ `ORDER`
* `summarise` ~ `aggregators: sum, min, sd, etc.`
* `mutate` ~ `operators: +, *, log, etc.`

```{r}
select(flights_tbl, year:day, arr_delay, dep_delay)

filter(flights_tbl, dep_delay > 1000)

arrange(flights_tbl, desc(dep_delay))

summarise(flights_tbl, mean_dep_delay = mean(dep_delay))

mutate(flights_tbl, speed = distance / air_time * 60)
```

## Preprocess

Use filter, select, arrange, and mutate to subset the flights data. Create a spark reference and a dplyr reference for the table.

```{r}
c4 <- flights_tbl %>%
  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%
  select(carrier, dep_delay, air_time, distance) %>%
  arrange(carrier) %>%
  mutate(air_time_hours = air_time / 60)

compute(c4, 'carrierhours')
carrierhours_tbl <- tbl(sc, carrierhours)
```

## Grouping

The `group_by` function corresponds to the `GROUP BY` statement in SQL.

```{r}
c4 %>%
  group_by(carrier) %>%
  summarize(count = as.numeric(n()), mean_dep_delay = mean(dep_delay))
```

## Collecting to R

You can copy data from Spark into R's memory by using `collect()`. 

```{r}
carrierhours <- collect(c4)
```

`collect()` executes the Spark query and returns the results to R for further analysis and visualization.

```{r}
# Test the significance of pairwise differences and plot the results
with(carrierhours, pairwise.t.test(air_time, carrier))
ggplot(carrierhours, aes(carrier, air_time_hours)) + geom_boxplot()
```

## Window Functions

dplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using `sql_render()`.

```{r, collapse=TRUE}
# Find the most and least delayed flight each day
bestworst <- flights_tbl %>%
  group_by(year, month, day) %>%
  select(dep_delay) %>% 
  filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))
sql_render(bestworst)
bestworst
```

```{r}
# Rank each flight within a daily
ranked <- flights_tbl %>%
  group_by(year, month, day) %>%
  select(dep_delay) %>% 
  mutate(rank = rank(desc(dep_delay)))
sql_render(ranked)
ranked
```

## Peforming Joins

```{r}
flights_tbl %>% inner_join(airlines_tbl)
flights_tbl %>% inner_join(airlines_tbl, by = "carrier")
flights_tbl %>% inner_join(airlines_tbl, by = c("carrier" = "carrier"))
```

## Sampling

You can use `sdf_sample` to take a random sample of rows.

```{r}
sdf_sample(flights_tbl, 0.01)
```

## Writing Data

It is often useful to save the results of your analysis or the tables that you have generated on your Spark cluster into persistent storage. The best option in many scenarios is to write the table out to a [Parquet](https://parquet.apache.org/) file using the [spark_write_parquet](reference/sparklyr/spark_write_parquet.html) function. For example:

* `spark_write_parquet` Writes a Parquet file
* `spark_write_csv` Writes a CSV file
* `spark_write_json` Writes a JSON file

```{r}
spark_write_parquet(flights_tbl, "nycflights13/parquet2")
```

## Reading Data

You can read data into Spark DataFrames using the following functions: 

* `spark_read_csv` Reads a CSV file and provides a data source compatible with dplyr
* `spark_read_json` Reads a JSON file and provides a data source compatible with dplyr
* `spark_read_parquet` Reads a parquet file and provides a data source compatible with dplyr

Regardless of the format of your data, Spark supports reading data from a variety of different data sources. These include data stored on HDFS (`hdfs://` protocol), Amazon S3 (`s3n://` protocol), or local files available to the Spark worker nodes (`file://` protocol) 

Each of these functions returns a reference to a Spark DataFrame which can be used as a dplyr table (`tbl`).

This will write the Spark DataFrame referenced by the tbl R variable to the given HDFS path. You can use the [spark_read_parquet](reference/sparklyr/spark_read_parquet.html) function to read the same table back into a subsequent Spark session:

```{r}
spark_read_parquet(sc, "data", "nycflights13/parquet2")
```

